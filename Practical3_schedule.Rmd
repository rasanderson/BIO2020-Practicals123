---
title: 'Practical 3: Multiple explanatory variables'
author: "BIO2020"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(bio2020)
knitr::opts_chunk$set(echo = TRUE)
pea_dat <- read.csv("Data/Peas.csv")
```

# Introduction
Now that we have developed a good understanding of linear models we are going to look at some more complex cases. The overall aim is to ensure that you are familiar with the use of linear models and the `lm()` function when you have multiple explanatory variables. This might arise in 'block' designed experiments, mixtures of continuous and categorical explanatories, and curve fitting. You will also have the opportunity to gain more confidence and expertise in manipulating and plotting your data. Specific objectives are to:

1. Explore the use of blocked experimental design, and how to analyse them
2. Understand interaction terms, and how to interpret them
3. Fitting curves to your data with `lm()`
4. Demonstrate how to manage unbalanced designs
5. Allow you to explore some of these ideas with new datasets (optional)


# 1. Blocked experiments
## 1.1 Refresher on basic concepts
Recall from [Interactive Website on Blocking](https://naturalandenvironmentalscience.shinyapps.io/multiple_explan/#section-blocking) that we use block designs to increase our ability to detect a treatment effect. In the linear models we have looked at so far, there has only been one explanatory variable, so we can think of it as:

$$Response = Explanatory + \epsilon$$
where:

* $Explanatory$ = your treatment, e.g. fertiliser (control, nitrogen, phosphorous)
* $Response$ = dependent variable, e.g. crop growth as a result of fertiliser
* $\epsilon$ = Greek letter Epsilon which stands for the unknown noise or variation in your data

If you are doing a field or laboratory experiment a common design is the **fully randomised design**. For example, you might have:

* 3 types of antibiotic being tested. You have 5 replicate Petri dishes for each antibiotic, and you randomly assign each antibiotic to one of the Petri dishes. You put all 15 Petri dishes on a tray into the incubator.
* 4 varieties of tomato plant are being grown, and you want to measure the chlorophyll content of their leaves. You grow up 6 replicate seedlings of each variety, randomly put them into different pots in a glasshouse
* 3 fertilisers plus a control (4 treatment levels) and you want to understand their effects on growth of pea plants. You create 6 plots (3 m x 3m) for each replicate giving 24 replicates in total, and allocate the fertilisers at random and later assess the growth of the pea plants.

Now there is nothing wrong with these fully randomised designs, but the problem is that **they are at greater risk of going wrong**. Purely by chance, you might end up with:

* most of the Petri dishes for one of your 3 antibiotics being near the door of the incubator. These get different and more variable temperatures than those at the back of the incubator.
* the temperature and humidity in the glasshouse are likely to differ on the north and south side of the glasshouse. It might be the one of your four varieties of tomato plants ends up on the north side, and grows more slowly
* there will be natural variation in the soil conditions across the twenty-four 3 m x 3 m plots (216 $m^2$ total area) where you grow your pea plants. If one of your fertiliser treatments ends up mainly on the poor, waterlogged soil this will bias your results.

So we recognise that there is natural variation in your "experimental arena" whether it is a laboratory, glasshouse or field experiment. A fully randomised design is vulnerable to this variation biasing your results. To solve the problem we divide the experimental arena into blocks, and randomly allocate our replicate treatments within each block:

* Divide your laboratory tray into 5 strips with marker pen, arranged so that when you put the tray into the incubator one strip is near the door, the fifth strip near the back of the incubator. Have 3 Petri dishes in each strip, and randomly allocate one of your 3 antibiotics to each of the three Petri dishes.
* Divide your glasshouse into 6 bands from north to south. Within each band, randomly place 4 plant pots containing your four different varieties of tomato.
* Divide your plot experiment with your peas into 6 areas, encompassing everything from the waterlogged soil to the drier soil. Within each area, mark out 4 plots, and randomly allocate the different fertiliser treatments.

These designs are known as **randomised block designs**. You are still allocating treatments to your replicates at random (essential), but you are doing so to **minimise systematic errors** due to so sort of variation in the "experimental arena". You can adopt a similar approach even for ecological field surveys, although it is harder to implement in practice. You can now modify your linear model to:

$$Response = Explanatory + Block + \epsilon$$

The advantage of including an explicit term called $Block$ into your linear model is that it makes the  error or noise ($\epsilon$) term smaller. The bigger the signal-to-noise ratio, which is what your F-ratio measures, the more likely you are to find out whether your $Explanatory$ treatment is doing anything to your $Response$ variable.

## 1.2 Example of a blocked experiment
We will start by using some data that comes from the pea growth and fertiliser experiment mentioned above. The data are available on Canvas in the `Peas.csv` file which you should save to your `Data` folder. Use the `read.csv()` function that your are hopefully now familiar with, import it into `pea_dat`, and use some of  the functions that you have learned about in previous practical sessions to explore the data.
 
From your inspections of the data what can you learn?
 
### 1.2.1 Lets talk about the 'shape' of your data
Often when you come to analyse some data you find that the data are stored in a format that is not quite what you want for analysis.
In such instances you may need to manipulate your data or reshape it so that you can carry out your analyses or create your plots.
This course is primarily focused on understanding the statistical principles that will enable you to rigorously inspect data that you come across rather than on data manipulation in R.
That being said it's all very well knowing how to analyse your data but if you can't get your data into the correct format you aren't going to be able to carry out your analyses.
We will walk you through the transformation here and if you want to learn more data manipulation skills you can work through an online tutorial such as [this](https://datacarpentry.org/R-ecology-lesson/03-dplyr.html).
The data are currently in what is referred to as `wide format` 
Use the `head()` function to look at the structure of the data as we currently have it.

```{r echo = FALSE}
head(pea_dat)
```

We have columns containing the yield values resulting from each treatment. This is the easiest way to enter the data into Microsoft Excel, so you will often record or receive data like this.
To compare the effects of the treatments on the yield values it would be more useful to have the yield values as a column alongside a column detailing the treatment.
This is what is referred to as `long format` data where each row contains a single observation. 


```{r echo = FALSE}
knitr::include_graphics("images/Pivot_longer.png")
```

You can convert data to a long format via the `pivot_longer()` function (`r emo::ji("package")` `tidyr`) . The `tidyr` package is installed by both the `mosaic` and `bio2020` packages, but you need to load it separately with `library(tidyr)`. We are going to use the `pivot_longer()` function here to switch our data set from `wide` to `long`. This will create a new object called `pea_dat_long`. We will retain the original format of the data unchanged in `pea_dat` which you should look at for comparison.

*Remember* you can inspect the help file for the function in the help tab, this will help you to understand what arguments the function is expecting. Simply type `?pivot_longer` in the Console window to view the help.

```{r, warning=FALSE, message=FALSE}
# remember to load the tidyr package
library(bio2020)
library(tidyr)

# reshape the data
pea_dat_long <- pivot_longer(
  data = pea_dat,
  cols = NitroGrow:Control,
  names_to = "Treatment",
  values_to = "Yield"
)
```

There are four `arguments` to the `pivot_longer()` function:

* `data` This is the name of your original "wide" dataset. The word `data=` is optional as the function assumes this will be the first term.
* `cols` These are the columns that we want to stack up one on top of another to create a new named column. The `:` symbol indicates that we want to go from the column called `NitroGrow` all the way consecutively to the column called `Control`
* `names_to` This is the name we want to give to a new column that will identify whether we have a record from `NitroGrow`, `PowerGro`, `Nitro.Power` or `Control`. These are the different levels of our new, single "explanatory" variable, so we will call it `"Treatment"` (double-quotes needed)
* `values_to` which are the actual numbers of the yield of peas, will be stacked into a new column, which we will call `"Yield"` (double-quotes needed)

You might have spotted that the fifth column, `Block`,in our original "wide" dataset `pea_dat` is not mentioned in the `pivot_longer()` function. Since we omitted it, R will leave it unchanged, but automatically fill out the number of repetitions needed. To look at your long set of data, either enter `View(pea_dat_long)` in the R Console window, or double-click on its name in the Envionment window.


### 1.2.2 Simple exploration of pea data
Now that you have your data in long format you should be able to calculate some simple statistics and visualise it before going ahead with an analysis via a linear model. Do some initial queries to check:

* Overall mean yield across all the plots; use
  + `summary(pea_dat_long)` which also gives minimum, maximum etc., 
  + `mean(Yield ~ NULL, data=pea_dat_long)` or
  + `mean(~Yield, data=pea_dat_long)`
* Standard deviation of `Yield`
* Standard error of `Yield` (**hint**: look back at practical 1)

**Question** When you used the `summary()` function, what did it return for the `Block` variable and the `Treatment` variable?

See if you can produce a boxplot similar to this using the `gf_boxplot()` function, combined with `%>%`, `gf_labs()` and `gf_refine()` with `theme_classic()`. Build it up line-by-line if unsure:

```{r, echo=FALSE}
gf_boxplot(Yield ~ Treatment, data=pea_dat_long) %>% 
  gf_labs(x = "Experimental treatment", y = "Yield of peas per plot (g)") %>% 
  gf_refine(theme_classic())

```

If you are feeling confident, can you remember how to make a violin plot similar to this? **Hint** Replace `gf_boxplot()` with `gf_violin()`, and use the `draw_quantiles = 0.5` option to add the median and `colour = ~Treatment` for colour-coding. The `gf_sina()` function adds the raw data points.

```{r, echo=FALSE}
gf_violin(Yield ~ Treatment, colour = ~Treatment, data=pea_dat_long, draw_quantiles = 0.5) %>% 
  gf_labs(x = "Experimental treatment", y = "Yield of peas per plot (g)") %>% 
  gf_sina() %>% 
  gf_refine(theme_classic())

```


## 1.3 Linear models of the pea data
### 1.3.1 Recoding `Block` and `Treatment`
You may have noticed that when you ran the `summary()` function earlier it returned the minimum, maximum, average etc. of your `Block` variable. This is of course nonsensical. You could just as easily have coded your six blocks A, B, C, D, E and F. Unfortunately, if you leave `Block` unchanged, R will assume it is a continuous variable, and that `Block 6` is "bigger" than `Block 1`. Likewise, the "character" class for `Treatment` is a little confusing, as it does not show the individual fertiliser treatments. We can re-code both variables into factors, and re-run the `summary()` function to see the difference. Remember that the `$` symbol allows us to access individual columns in a table ("data frame") and the `as.factor()` function tells R that the variable is categorical:

```{r}
pea_dat_long$Block <- as.factor(pea_dat_long$Block)
pea_dat_long$Treatment <- as.factor(pea_dat_long$Treatment)
summary(pea_dat_long)
```

That is much better. We no longer have silly statistics for the `Block` column, and the individual fertiliser treatments are now being displayed for the `Treatment`. What do the numbers `4` and `6` represent alongside each row of these entries?

### 1.3.2 Linear models of pea dataset with and without `Block`
We'll begin with a simple linear model that ignores the `Block` variable, just to demonstrate the improvement afterwards.

```{r}
pea_dat_lm1 <- lm(Yield ~ Treatment, data=pea_dat_long)
anova(pea_dat_lm1)
```

What do you conclude from this? Do the fertilisers have a significant effect? How would your report these data? Remember that by convention p=0.05 is usually the critical cut-off for "statistical significance" so what would be an appropriate way to report your results? Do a QQ plot of the residuals? Are assumptions of the model robust?

Now repeat but include `Block`. Notice that we are naming our second set of results `pea_dat_lm2`. This is a common convention in R, to end the name of the output with the name of the analysis (here `lm`) and if doing several models to add a suffix `1`, `2` etc. for clarity:


```{r}
pea_dat_lm2 <- lm(Yield ~ Treatment + Block, data=pea_dat_long)
anova(pea_dat_lm2)
```
 
Notice how the `Treatment` for your fertilisers is now significant. There are several points to notice here:

* The total Sums of Square (a measure of total variation in your dataset) in the data is unchanged in both `lm1` and `lm2` of your pea yield. See [This Interactive Website](https://naturalandenvironmentalscience.shinyapps.io/how_anova_works/#section-variances-with-categorical-explanatory-variables) to remind yourself of the different components of SS
* The **Residuals** are **much smaller** in `pea_dat_lm2`. Residuals are unexplained noise, or the $\epsilon$ in your data. The smaller you can make the noise, the more likely you are to detect a treatment effect.
* The SS is the same for your `Treatment` in both `pea_dat_lm` and `pea_dat_lm2`
* Variance explained by each term is measured in the column headed `Mean Sq` by dividing the `Sum sq` by `Df` (degrees of freedom)
* The `F value` for the fertiliser treatment is calculated by dividing its `Mean Sq` by that of the Residuals. The bigger the F-ratio the more significant (lower p-value), i.e.
   + F-ratio = Treatment Mean Sq / Residual Mean sq
   + `pea_dat_lm1`  F-value = 86.427 / 30.251 = 2.857 (non-significant)
   + `pea_dat_lm2`  F-value = 86.427 / 17.349 = 4.982 (significant)

So what changed was that we moved some of the variation in your data out of "noise" and into "Block". This allowed us to focus on what we were interested in, namely the fertiliser treatment. As before, check the residuals of your second linear model, and create a QQ plot.

### 1.3.2 Which fertiliser treatment is best?
Of course, the above analysis merely shows us that fertiliser has an effect on yield. We also want to know how one fertiliser compares with the others, as well as the Control. So we need to do a Tukey multiple comparison test, using the `TukeyHSD()` function. Try running the `TukeyHSD()` function on the `pea_dat_lm2` results. You'll see that it gives a huge amount of results, because not only does it compare all the pairwise Fertiliser combinations, it also does the same for all the Blocks. We are not interest in Block pairwise comparisons, so we can force the function to only look at the fertilisers via:

```{r, eval=FALSE}
TukeyHSD(pea_dat_lm2, which="Treatment")
```

Next, create a plot of the results of the Tukey test. Ask a demonstrator or member of staff if you are unsure how to interpret the results.

# 2. Interactions terms: when the explanatory variables are not independent
## 2.1 What are interaction terms?
Interaction terms can be useful in both designed experiments and field surveys. They provide a way of checking whether the effects of two explanatory variables on the response are independent of each other, or alternatively whether the value of one explanatory variable alters what the other one does.

## 2.2 Basic concepts in the 'goal-oriented' approach
Our original linear model, with two explanatory variables is

$$Response = \textit{Explanatory 1} + \textit{Explanatory 2} + \epsilon$$
and we revise this to:

$$Response = \textit{Explanatory 1} + \textit{Explanatory 2} + \textit{Interaction} + \epsilon$$
 
where:

* $Response$ = dependent variable, e.g. crop growth as a result of fertiliser
* $\textit{Explanatory 1}$ = your first treatment, e.g. fertiliser (control, nitrogen, phosphorous)
* $\textit{Explanatory 2}$ = your second treatment, e.g. pesticide (control, insecticide)
* $\textit{Interaction}$ = measures how response variable changes as a result of **both** first and second treatment
* $\epsilon$ = Greek letter epsilon = the unexplained "noise" in your data
 
The $\textit{Explanatory 1}$ and $\textit{Explanatory 2}$ variables can be continuous and/or categorical. You can express an interaction term in R using the `lm()` function, by adding an extra term with the two explanatories separated by a colon `:` symbol. Thus, if your response was `yield`, your two explanatories were `fertiliser` and `pesticide` in a table of data called `crop_data` you would write:

`crop_lm <- lm(yield ~ fertiliser + pesticide + fertiliser:pesticide, data=crop_data)`

Let's look at the example from the [Interactive Website on interaction terms](https://naturalandenvironmentalscience.shinyapps.io/multiple_explan/#section-interactions-between-explanatory-variables)

## 2.3 Example of interactions: blood plasma calcium in rabbits
Let's look at the example of the blood Ca level in rabbits, half from a lowland agricultural farm, half from an upland farm, split according to gender. Download the file `plasma.csv` from Canvas and put it into your **`Data`** folder. Using the `read.csv()` function, import the file into an R table called `plasma_dat`. Now see if you can produce a boxplot similar to the following. **Hints**:

* `gf_boxplot()` is main function
* `gf_labs()` to change default labels
* `gf_refine()` to change overall format of plot

```{r, echo=FALSE}
plasma_dat <- read.csv("Data/plasma.csv")
gf_boxplot(calcium ~ site, colour = ~sex, data=plasma_dat) %>% 
  gf_labs(y = "Blood calcium (mg / 100 ml)", x = "Farm location") %>% 
  gf_refine(theme_classic())

```

 From the plot you can see several trends:
 
 * males seem to have lower blood Ca than females overall
 * in males the blood Ca increases from lowland to upland, whereas for females it decreases
 * the differences between males and females appear to be bigger at lowland than upland farms
 
 You can now create the linear model using `lm()` as usual, but include an interaction term:
 
```{r}
calcium_lm <- lm(calcium ~ site + sex + site:sex, data=plasma_dat)
anova(calcium_lm)
```
 
Notice how your ANOVA table now has **three** rows for the explanatories, namely `site` on the first row, `sex` on the second row, and the interaction term `site:sex` on the third row. When `site` and `sex` are on their own, as on the first and second row, they are referred to as **"main effects"** to distinguish them from when they both occur together in the third row as the interaction term. As usual there is of course a final row for the `Residuals` or unexplained noise ($\epsilon$) in your data. You can see three F-ratios (one for each explanatory variable) and associated p-values, the latter under the column headed `Pr(>F)`.

Before reading on, think about these questions:

* How would you report the above F and p-values in a report?
* Which explanatory variables are "statistically significant"?

## 2.4 How to interpret a linear model with interaction terms
When you have an interaction term as one of your explanatory variables, always look at it **first**. If it is not significant, you might be able to manage with a simpler linear model that does not include interaction terms. It is always best to try and have a simpler rather than overly complex linear model when possible.

However, in this example the interaction term **is** significant. Indeed, you have:

* `site` main effect. $F_{1,16}=0.017, p=0.899$ non-significant
* `sex` main effect. $F_{1,16}=167.658, p<0.001$
* `site:sex` interaction. $F_{1,16}=47.604, p<0.001$

So you can see that both the `site:sex` interaction and `sex` main effect terms are highly significant, whereas the `site` main effect is non-significant. This type of result is not uncommon, but at first glance is very confusing. Why is the `site` main effect unimportant, whilst it seems to have a big impact on blood calcium in the interaction? This seems a little contradictory.

The easiest way to understand the process is to plot individual graphs for each component. See if you can create these graphs on your own.

First of all the `site` main effect:

```{r, echo=FALSE}
gf_boxplot(calcium ~ site, data=plasma_dat)

```

It is fairly obvious that the overall amounts of blood calcium are fairly similar in both the lowland and upland farms, if we ignore gender, although the range of values is smaller in the uplands. A boxplot shows the median as the middle horizontal line (try calculating the means for comparison) and these are quite close to each other. This explains the non-significant `site` main effect.

Second, the sex main effect. Try and produce a graph similar to the following:

```{r, echo=FALSE}
gf_boxplot(calcium ~ sex, data=plasma_dat)
```

Now the sets of values are very different, with the males much lower overall than the females, which explains the large F-ratio and highly significant (p<0.001) results for `sex` main effect.

Finally, the interaction term. As this graph is a little trickier to draw, I have included the R code. We include both the raw data, and lines connecting the means, to show the direction of change:

```{r}
# gf_point() adds the raw data points
# gf_line() adds lines. We use stat="summary" to indicate the mean
gf_point(calcium ~ site, colour= ~sex, data=plasma_dat) %>%
  gf_line(calcium ~ site, colour= ~sex, group= ~sex, stat="summary", data=plasma_dat)
```

The important point to note here is that **the lines are not parallel**. If the lines were parallel, either upwards or downwards, it would indicate that the blood calcium changed in a similar way for both genders when moving from lowland to upland farms. However in reality they are not parallel, indeed the differences are so big that the gradients of the lines go in opposite directions. This indicates that the physiology of male and female rabbits in response to the elevation change is not the same.

To explore these data more [look at this interactive webpage](https://naturalandenvironmentalscience.shinyapps.io/multiple_explan/#section-interactive-demonstration). This uses the same data, but you can randomly adjust some of the terms, and see how the results change. Begin by setting the interaction term to zero, and notice the difference.

# 3. Using linear models to fit curves
## 3.1 Data transformation or polynomials?
It is much easier to fit a straight line than a curve, but often an initial exploration of your data by plotting it will reveal that a straight-line does not provide a satisfactory model. The two main approaches to resolve this are to:

1. A simple mathematical transformation of your data to bring it back to a straight line. Commonly used transformations are logarithmic `log()` and square-root `sqrt()` of your response variable.
2. Use a polynomial linear model, by adding extra explanatory variables calculated from the first explanatory variable. The explanatories thus become $x$, $x^2$, $x^3$ etc.

The use of both methods is somewhat controversial. Mathematical transformations can make it harder to understand and interpret the results, especially if there is not an obvious biological basis for the transformation. High-order polynomial terms such as $x^3$ and $x^4$ will create attractive smooth curves, but have little biological meaning. Conversely, some mathematical transformations are so common you probably never even think about them: **pH** is actually the log-transformation of hydrogen ions; these have a skewed 'log-normal' distribution by default, so taking a logarithm to create the pH scale makes sense. You can also end up with skewed data when you have counts of a response variable, since you cannot have negative counts. It used to be common to log-transform count data using `log(y+1)` (in case there are any zeros). Here we will focus on a polynomial example to fit a curve.

## 3.2 Polynomial linear model: quadratic for simple curves
The word "polynomial" simply indicates that we are going to derive new $x^2$ (x-squared), $x^3$ (x-cubed) etc. terms to put into our linear model in order to fit a curve. Due to problems of biological interpretation, it is very rare to go beyond a simple **quadratic** model:

$$\textit{Response variable}=\textit{Explanatory variable}+\textit{Explanatory variable}^2 + \epsilon$$
where

* $\textit{Response variable}$ = dependent, y-axis variable
* $\textit{Explanatory variable}$ and $\textit{Explanatory variable}^2$ = independent, x-axis variable, either in its original form, or squared.
* $\epsilon$ = Greek letter epsilon for unknown noise or variation in your data.

These are easy to create using the `lm()` function using:

`lm(response ~ explanatory + I(explanatory^2), data=dataset_name)`

The slightly confusing change to the usual syntax is that `^2` is used to indicate that we are going to add a squared explanatory, and we have to "wrap" the squared term in an extra set of brackets prefaced by `I()` to indicate that we "Intend" to do this, and it isn't a typing error.

## 3.3 _Lolium_ growth example
Let us build on the example at the [Interactive Website on Curve fitting](https://naturalandenvironmentalscience.shinyapps.io/multiple_explan/#section-fitting-curves) for growth of the grass _Lolium perenne_ in soils of different water content. You will need to first:

* dowload the file `grass_growth.csv` from Canvas
* copy `grass_growth.csv` into your `Data` folder
* import it into R using `read.csv()` as a table called `lolium_dat`

Use the `head()` function to look at the first few rows, or double-click on `lolium_dat` in the "Environment" pane (top-right of RStudio) to view the whole table of data. Next, produce a scatterplot of the data:

```{r, echo=FALSE}
lolium_dat <- read.csv("Data/grass_growth.csv")
```

```{r, echo=TRUE}
gf_point(growth ~ water, data=lolium_dat) %>% 
  gf_labs(x = "Soil water content (%)", y="Growth rate") %>% 
  gf_theme(theme_classic())
```

You can see that whilst the pattern of points is broadly increasing, but beyond about 75% soil water it starts to drop sharply. _Lolium_ is not an aquatic plant, so this makes sense biologically of course. 

### 3.3.1 Simple (straight line) linear model
Let's look at a simple straight-line linear model to begin. This model is of the form:

$growth = water + \epsilon$

```{r, echo=TRUE}
lolium_lm1 <- lm(growth ~ water, data = lolium_dat)
summary(lolium_lm1)
```

So that was easy! You have a highly significant linear model, showing that _Lolium_ growth increases with water content. The overall model is significant with $F_{1,17}=11.45, p=0.004$, and the $R^2=0.367$ (model explains 36.7% of variation). But... we should always check model assumptions by looking at the residuals to see if there are any abnormalities.

### 3.3.2 Diagnostics for simple (straight line) linear model
We can actually produce 4 "diagnostic" plots quite easily with the `plot()` function, writing `plot(lolium_lm1)`. Only the first two are really useful here. The `plot()` function extracts the residuals and creates a QQ plot automatically. All the plots look at "Residuals". Remember residuals are the differences between your fitted straight line and the observed points.

```{r}
plot(lolium_lm1, which=1:2) # We only want the first two diagnostic plots
```

* The first plot is called **Residuals vs Fitted**. Notice how it bends down at both low and high fitted values, with the residuals drifting away from zero, whereas ideally the scatter around should be fairly constant.
* The second plot, titled **Normal Q-Q** is the quantile-quantile plot you have created manually before. Notice how poorly the points align with the expected dotted line, especially at the extremes.

### 3.3.3 Add the fitted line to see the problem
If you modify your plot, by adding a line via the `gf_lm()` function, the problem is obvious:

```{r, echo=TRUE}
gf_point(growth ~ water, data=lolium_dat) %>% 
  gf_labs(x = "Soil water content (%)", y="Growth rate") %>% 
  gf_lm() %>% 
  gf_theme(theme_classic())
```

Thus, by looking at the raw data, especially when you overlay the fitted line, as well as the diagnostic residual plots from the model, as well as basic plant biology, you can see this is not a good model. Let's change it to a curve, by changing it to a quadratic.

### 3.3.4 Quadratic (curve) linear model
This means our model is now of the form:

$growth = water + water^2 + \epsilon$

which we can express in R as:

```{r, echo=TRUE}
lolium_lm2 <- lm(growth ~ water + I(water^2), data = lolium_dat)
summary(lolium_lm2)
```

This linear model is also significant. You will notice that the overall model is highly significant with $F_{2,16}=64.37,p<0.001$ and $R^2=0.876$, or 87.6% explained. **Note** In the previous sentence the p-value is written as $p<0.001$ which indicates "p is less than 0.001". This is because in the overall linear model output it the p-value is shown as `p-value: 2.23e-08` which is the same as $p=0.0000000223$. The convention is that for tiny p-values, the actual amount is not shown, and instead the "less than 0.001" syntax is used.

Is the improvement in the overall model of `lolium_lm2` compared to the earlier `lolium_lm1` significant? We can compare the two models using `anova()`:

```{r}
anova(lolium_lm1, lolium_lm2)
```

This is a test to compare our two linear models, and it gives $F_{1,16}=70.48, p<0.001$ indicating that there is a big difference in the performance of our two models, with our second one, labelled `Model 2` in the above output, better. Notice also the column headed `RSS` which stands for "Residual Sum of Squares". This is a measure of the "noise" that is not explained by our model. You can see that it is much smaller at 146.81 for the second model.

### 3.3.5 Coefficients from quadratic model `lolium_lm2`
Look at the column headed `Estimate` in the `summary(lolium_lm2)` above. All three estimates, the `(Intercept)`, the `water` and quadratic `I(water^2)` terms are significant, but their values are very different, at 10.716, 1.067 and -0.009 respectively. It is common for the absolute value of the squared term to be very small. **Questions**:

* Why is the value of the estimate for the squared term so small? Why might this be? 
* Will the squared term always have a negative value?
* When might it be positive?

Ask on Microsoft Teams if unsure, under the "Section 3 - Dealing with multiple explanatories" channel.

### 3.3.6 Diagnostics and plots from quadratic model `lolium_lm2`
Replot the first two diagnostic from `lolium_lm2` and you can see that both of these are improved compared to the original.

```{r, echo=FALSE}
plot(lolium_lm2, which=1:2)
```

Finally, let's plot the raw data, plus our quadratic model with confidence intervals. By default, the `gf_lm()` function adds a straight line, but if we give it the same quadratic we used in the model, it will add a curve.

```{r}
gf_point(growth ~ water, data = lolium_dat) %>% 
  gf_lm(formula = y ~ x + I(x^2), interval = "confidence")
```

This looks much better and more realistic. It is a better statistical model, and also makes more biological sense. Before you go further, try modifying this plot so that the horizontal axis is labelled "Soil wated %" and the vertical axis "Growth rate".

# 4. Unbalanced designs
Unbalanced designs occur regularly in biology - the term 'unbalanced' simply means that you don't have the same number of replicates in each of your treatment levels. This can make the interpretation of the analyses a little trickier. Ideally you would have the same number of replicates in every treatment level, but sometimes for practical reasons this may not be possible. Experimental error may also cause problems, e.g.

* contamination of some Petri dishes in a laboratory experiment
* death of some plants in a glasshouse experiment
* vandalism of some experimental plots in a field experiment (this has happened to me!)

## 4.1 Revisit the blood calcium rabbits dataset
An easy way to demonstrate the problem that unbalanced data causes is to go back to our Ca in rabbits example, but deliberately remove one row of data to make it unbalanced. Perhaps one of our tagged rabbits was eaten by a fox...

In the R commands below, the `[]` syntax allows us to refer to individual rows and columns of a table. For example, `plasma_dat[3, 2]` would find the number in the 3rd row and 2nd column of the table of data in `plamsa_dat`. The description `plasma_dat[-4,]` means "omit the 4th row". As we did not add anything after the `,` it keeps all the columns. Have a look at the [Data Frames](https://naturalandenvironmentalscience.shinyapps.io/RstudioInstall/#section-working-with-data) section of the Interactive Website as a refresher.

```{r, eval=FALSE}
# Omit the 4th rabbit to create an unbalanced dataset
plasma_dat_unbal <- plasma_dat[-4,]


# Compare the number of records in each replicate
tally(~ site + sex, data=plasma_dat) # Original
tally(~ site + sex, data=plasma_dat_unbal) # Unbalanced
```

The `tally()` function counts up the numbers of replicates in each of the 4 combinations of explanatory variables, and whereas the original data shows you 5 replicates in every combination, in the second `plasma_dat_unbal` dataset (greedy fox), there are only 4 upland females. **Note**: we don't need to include a `calcium` response variable on the left of the `tally()` function, since we are focussing on the explanatories.

Now repeat the earlier analysis from "2.3 Example of interactions" but with the unbalanced data. Do the analyses with the interaction term last, but change the order of the main effects:

```{r, eval=FALSE}
plasma_unbal_lm1 <- lm(calcium ~ sex + site + sex:site, data=plasma_dat_unbal)
plasma_unbal_lm2 <- lm(calcium ~ site + sex + sex:site, data=plasma_dat_unbal)
anova(plasma_unbal_lm1)
anova(plasma_unbal_lm2)
```

The two analyses produce identical results for the `site:sex` interaction term, but not for the `site` and `sex` main effects. This is common with unbalanced data, and sometimes variables can change from being significant to non-significant. **Repeat**: Try the same two analyses again with the original balanced `plasma_dat` dataset and you will see that in contrast the order in which you enter the variables has no effect.

## 4.2 Using different Sums of Squares (SS) with unbalanced data
By default R uses "sequential" sums of squares (SS) when calculating the values for an ANOVA table. Sequential SS are sometimes called "Type I" tests. There are several different methods of calculating SS, all with their advantages and disadvantages. Sequential SS is usually best with balanced data, but does not work properly with unbalanced data. See [this recent paper](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.1365-2656.2009.01634.x) for a comparison of the different types of SS from a biological perspective.

The easiest correction is to apply "adjusted" sums of squares, which give the same result irrespective of the order in which you enter the variables. This is available via the `Anova()` function (captial letter A), which is in the `car` package. If you successfully installed and have already loaded the `bio2020` package via `library(bio2020)` then `car` will already be loaded for you. If you have not installed `bio2020` then do `library(car)` to access the `Anova()` function. If you receive a message stating "package car not found" then you need to install it in the usual way. Try the following:

```{r, eval=FALSE}
Anova(plasma_unbal_lm1)
Anova(plasma_unbal_lm2)
```

You can now see that it does not matter what order you put the explanatories into your linear model, and the result is the same. **Note**: This problem can sometimes arise if you have a mixture of continuous and categorical explanatory variables, and therefore you may wish to use `Anova()` rather than `anova()` in those circumstances.

You might be wondering, "Why not use Type II tests all the time?". Some statistics packages do indeed use Type II tests as their default, and there is nothing wrong with them. However, if you have balanced data, Type II tests tend to be slightly less powerful. In other words, they are not quite as good at detecting a treatment effect compared to the default Type I test. Of course, this depends on you having balanced data!

# 5. "Data wrangling" large datasets
It is likely that many of you will have to deal with large "messy" datasets at some point that need to be "cleaned" or pushed through "data wrangling" before you can analyse them. Classic examples might include:

* DNA sequence data. Often there are blank reads or errors, even on modern high-throughput devices such as Oxford Nanopore MinIon methods.
* Environmental data. Automatic devices to measure the environment, such as temperature, relative humidity etc., generate large amounts of useful data, but often contain 'blanks' or missing data due to short-term equipment failures
* Camera trap data. These are excellent for monitoring animals in the field, generating large quantities of data without interferring with animal behaviour. Unfortunately images are often blurred, unreadable, batteries can fail, or camera traps may be stolen.


## 5.1 Pantheria database
As an example of a large dataset which can be tidied up or subset, we will look at the Pantheria dataset of carnivorous mammal weights. Download the file `Pantheria.csv` from Canvas, which contains information from the [Pantheria database](https://ecologicaldata.org/wiki/pantheria). This is a huge dataset with over 100,000 records and 59 columns, so only a small part of it (422 rows and 8 columns) is provided to avoid swamping your disk quota. The full dataset includes living and extinct species of the order Carnivora, with information about their geographic extent, physical size, life history, ecological characteristics etc. It has been used to try and understand why some species of mammals are at greater risk of extinction than others.

As usual, download the `Pantheria.csv` file from Canvas, then copy this into your **`Data`** folder, and import it into R/RStudio using the `read.csv()` function. 

```{r echo = TRUE}
pantheria <- read.csv("Data/Pantheria.csv")
summary(pantheria)
```

After reading in and summarising the data, the column names are:

* **X** The first column in the CSV file is the record number. If you look at the .CSV file you'll notice that this row does not actually have a heading, therefore R has automatically added the heading `X`.
* **Common_name** The common (English) name of the carnivore.
* **Family** The Latin family name. Remember that under the scientific classification system, family-level names all end in "dae"
* **HB_max** The maximum head : body length ratio recorded for that species
* **W_max** Maximum recorded weight
* **Skull_max** Maximum size of skull
* **HomeR_max** Maximum home range of the predator
* **Density_max** Maximum recorded population density of the predator

The output from the `summary()` above gives the names columns, and numeric summaries of some columns. You will see that two columns, `Common_name` and `Family` are listed as `Class :character`. You might find it easier to change these into "factors" and re-run the `summary()` function. Look back to **Part 1.3.1** where you re-coded variables from the pea growth dataset to remind yourself how to do this. Check how many rows and columns of data you have either by checking in the Environment pane of RStudio, or using the `dim(pantheria)` command. You can see this is quite a big dataset.

Sometimes you will only want to study a small part of a big dataset, so you need to subset or "filter" it to obtain that part. This can be where we have more data than we want to investigate or plot, in these instances it can be useful to extract only a subset of our data.
The pantheria data for example contains species from several families of carnivores but you might only be interested in one family of animals.
In such instances we can use the `filter` function from the `dplyr` package to extract only the subset that we are interested in. The `dplyr` package is automatically loaded for you by `mosaic` or by `bio2020`.

For example if we wanted to create a data frame that contained only information regarding the Canidae (dog) family we could do the following:

```{r,warning= FALSE, message = FALSE}
# Only keep the records where the Family are dogs, or Canidae
dog_dat <- filter(pantheria, Family == "Canidae")
```

**Note:** You might be wondering why it says `Family == "Canidae"` rather than `Family = "Canidae"`. The double-equals `==` symbol is used for comparisons. The following table summarises some of the more useful comparisons:

|Symbol|Meaning|Example|Rows returned|
|:-----:|:-----:|:-----:|:------------|
| `==`| equal to| `Family == "Canidae"`| only dogs returned|
| `!=` | not equal to | `Family != "Canidae"`| all the records except dogs|
| `>` | greater than| `HomeR_max > 20` | taxa a home range greater than 20 m|
| `<` | less than|  `HomeR_max < 20` | taxa with a home range less than 20 m|
| `>=` |greater than or equal to| `HomeR_max >= 20`| taxa with a home range of 20 m or above|
|`<=`|  less than or equal to | `HomeR_max <= 20`| taxa with home range of 20 m or less|
| `&`| and| `Family == "Canidae" & Skull_max < 15|`|  dogs with skulls less than 15 cm|


## 5.2 Cleaning up missing or `NA` values
The presence of any rows with missing values can cause problems, as if you attempt to do any calculations on them, you will receive `NA` as an output. If you direct the results of your `filter` into the `na.exclude()` function using the pipe symbol `%>%` any rows with missing data will be removed. For example, compare:

```{r echo=FALSE}
pantheria$Common_name <- as.factor(pantheria$Common_name)
pantheria$Family <- as.factor(pantheria$Family)
```
```{r}
dog_dat <- filter(pantheria, Family == "Canidae")
summary(dog_dat)
```

with

```{r}
dog_dat <- filter(pantheria, Family == "Canidae") %>% 
  na.exclude()
summary(dog_dat)
```

**Note** If the information displayed for `Common_name` and `Family` shows as `character` it is because you haven't converted them to factors. See Part 1.3.1 earlier.

## 5.3 Selecting columns
The full Pantheria dataset has almost 60 columns. If you do struggle with a large dataset, it can be useful to be able to extract only some columns to work with. For example, if you only wanted the `Common_name`, `Family`, `HB_max` and `W_max` columns use the `select()` function:

```{r}
four_columns <- select(pantheria, Common_name:W_max)
summary(four_columns)
```

If the columns you want are not next to each other, name them individually within the `c()` function. e.g. to extract `Common_name`, `Family`, `HB_max` and `Density_max`

```{r}
four_columns <- select(pantheria, c(Common_name:HB_max, Density_max))
summary(four_columns)
```

## 5.4 Explore in your own time
* Choose a family to investigate, create a new data frame containing only records from that family using the `filter()` function. Remove missing values using `na.exclude()`
* Create a plot of the data that shows the relationship between head-body length and skull length for species in your chosen Family